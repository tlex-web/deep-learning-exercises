{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "9jiIATcpFsPc"
   },
   "source": [
    "### Group Members:\n",
    "\n",
    "\n",
    "- Tim Lui, 24-755-092\n",
    "- Boris Trifonov, 24-750-077"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "FcStVd7x1zL3"
   },
   "source": [
    "# Assignment 2: Binary and Categorical Classification\n",
    "\n",
    "For this assignment, we will use a similar two-layer network as Assignment 1 to implement binary and categorical classification using only `NumPy` package, without relying on `PyTorch` or any deep learning libraries.\n",
    "\n",
    "We will implement Binary Cross Entropy Loss and Categorical Cross Entropy Loss and their gradient manually with efficient matrix computation, and evaluate accuracy for two classification tasks.  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "sIA1V8sU2W2W"
   },
   "source": [
    "## Theoretical Questions\n",
    "\n",
    "We will build and train a two-layer network similar to Assignment 1.\n",
    "Besides, we will apply **Logistic Function** ($\\sigma$) on the logits for binary classification and **Stable Softmax Function** ($\\tilde{\\mathcal S}$) on the logits for categorical classification.\n",
    "\n",
    "Questions in this section concern:\n",
    "\n",
    "1. Comparison between normal softmax $\\mathcal S$ and stable softmax $\\tilde{\\mathcal S}$\n",
    "2. Compute derivative of $\\tilde{\\mathcal S}$ that should be applied in the coding section\n",
    "3. Weight/Parameter Initialization\n",
    "4. Discussion on Evaluation Metrics of classification tasks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "DmECZG_U39zs"
   },
   "source": [
    "### Task 1.1 Proof of Stable Softmax\n",
    "\n",
    "The softmax function is defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal S(z_o) = \\frac{e^{z_o}}{\\sum\\limits_{o'} e^{z_{o'}}}\n",
    "$$\n",
    "\n",
    "However, when $ z_o $ has large values, the exponential $ e^{z_o} $ may grow too large, leading to numerical instability (e.g., overflow issues).\n",
    "To improve stability, we introduce the **stable softmax**:\n",
    "\n",
    "$$\n",
    "\\tilde{\\mathcal S}(z_o) = \\frac{e^{z_o - z_{\\max}}}{\\sum\\limits_{o'} e^{z_{o'} - z_{\\max}}}\n",
    "$$\n",
    "\n",
    "where $z_{\\max} = \\max\\limits_o z_o$.\n",
    "\n",
    "Prove that stable softmax is mathematically equivalent to the original softmax function.\n",
    "\n",
    "Discuss how stable softmax improves numerical stability, for instance, what will happen when $z_o$ is too large, or too small?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "gElE0oDt4nTW"
   },
   "source": [
    "Proof:\n",
    "$\\tilde{\\mathcal S}(z_o) = \\frac{e^{z_o - z_{\\max}}}{\\sum\\limits_{o'} e^{z_{o'} - z_{\\max}}}$\n",
    "\n",
    "$\\tilde{\\mathcal S}(z_o) = \\frac{\\frac{e^{z_o}}{e^{z_{max}}}}{\\sum\\limits_{o'} \\frac{e^{z_o'}}{e^{z_{max}}}}$\n",
    "\n",
    "$\\tilde{\\mathcal S}(z_o) = \\frac{\\frac{e^{z_o}}{e^{z_{max}}}}{\\frac{1}{e^{z_{max}}} \\sum\\limits_{o'} e^{z_{o'}}}$\n",
    "\n",
    "$\\tilde{\\mathcal S}(z_o) = \\frac{e^{z_o}}{\\sum\\limits_{o'} e^{z_{o'}}} = \\mathcal S(z_o)$\n",
    "\n",
    "Discussion:\n",
    "   Large values of $z_o$: TODO\n",
    "   Small values of $z_o$: TODO\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "dF71qySq1qhn"
   },
   "source": [
    "### Task 1.2 Derivative of Stable Softmax\n",
    "\n",
    "\n",
    "Recall that in the lecture slides, when we compute gradient with respect to $w^{(1)}$ and $w^{(2)}$, as part of the chain rule, we replace $\\frac{\\partial\\mathcal J^{[n]}}{\\partial\\mathcal y_o^{[n]}}\\frac{\\partial\\mathcal y_o^{[n]}}{\\partial\\mathcal z_o^{[n]}}$ by $\\frac{\\partial\\mathcal J^{[n]}}{\\partial\\mathcal z_o^{[n]}}$ when we choose a good activation function.\n",
    "\n",
    "When normal softmax is applied, we have $\\frac{\\partial\\mathcal J^{[n]}}{\\partial\\mathcal z_o^{[n]}} = y_o^{[n]} - t_o^{[n]}$. Prove that $\\frac{\\partial\\mathcal J^{[n]}}{\\partial\\mathcal z_o^{[n]}}$ will return the same format when the activation function is stable softmax.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "bUlc_2ta5bF1"
   },
   "source": [
    "Answer: ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "O-QbnR__zCJB"
   },
   "source": [
    "### Task 1.3 Weight Initialization\n",
    "\n",
    "Why do we need to randomly initialize the weights before training starts?\n",
    "\n",
    "1. Proof that a network with constantly initialized weights perform like a network with one hidden unit.\n",
    "2. Show that gradient decent is not able to change this behavior and will keep all neurons in the hidden layer identical throughout training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "2cD9ZVUoznUE"
   },
   "source": [
    "Answer: ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "uBmFHEbvNoT7"
   },
   "source": [
    "### Task 1.4 Evaluation Metrics\n",
    "\n",
    "If a binary classifier achieves an accuracy greater than 90%, see example plot in the OLAT, does this always indicate good performance? Might high accuracy in this binary classification be misleading? \n",
    "\n",
    "What alternative evaluation approaches could better capture model performance in such scenarios?\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "DRDxQW4xy2E3"
   },
   "source": [
    "Answer: ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "hi5N_hI79qkF"
   },
   "source": [
    "## Coding\n",
    "\n",
    "**<font color='red' size='5'>This section has to be submitted by 11:59 p.m. on Wednesday, March 26, to be graded.</font>**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "E-n2m59B4iP8"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "We will use two different datasets, the *churn* dataset https://archive.ics.uci.edu/dataset/563/iranian+churn+dataset for a binary classification and the *winequality-red* dataset https://archive.ics.uci.edu/dataset/186/wine+quality for a categorical classification. Both datasets are available on the UCI Machine Learning repository.\n",
    "\n",
    "The binary classification dataset contains features extracted from customers of a telecommunication company, which are classified as either churn or not.\n",
    "The categorical classification dataset contains chemical measurements for seven distinct qualities of a Portuguese white wine.\n",
    "In the former dataset, the class is indicated in the final column named \"Churn\", whereas for the latter, target information is provided in the last column named \"quality.\"\n",
    "\n",
    "Please run the code block below to download the data files."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5Shd71QB4iP_",
    "ExecuteTime": {
     "end_time": "2025-03-18T10:22:38.576856Z",
     "start_time": "2025-03-18T10:22:38.564591Z"
    }
   },
   "source": [
    "import os\n",
    "import zipfile\n",
    "import urllib.request\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Download the two dataset files\n",
    "dataset_files = {\n",
    "    \"churn_data.zip\": \"https://archive.ics.uci.edu/static/public/563/iranian+churn+dataset.zip\",\n",
    "    \"winequality-red.csv\": \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "}\n",
    "\n",
    "for name, url in dataset_files.items():\n",
    "    base, extension = os.path.splitext(name)  # Get file name without extension\n",
    "\n",
    "    if os.path.exists(name):\n",
    "        print(f\"File '{name}' already exists. Skipping download.\")\n",
    "    else:\n",
    "        # Download the file\n",
    "        urllib.request.urlretrieve(url, name)\n",
    "        print(f\"Downloaded {name} successfully.\")\n",
    "\n",
    "    # Check if it's a zip file and if it has been extracted\n",
    "    if extension == \".zip\":\n",
    "        extracted_file = os.path.join(base + \".csv\")  # Expected extracted file name\n",
    "\n",
    "        if not os.path.exists(extracted_file):\n",
    "            with zipfile.ZipFile(name, 'r') as zip_ref:\n",
    "                zip_ref.extractall()\n",
    "            print(f\"Extracted {name} successfully.\")\n",
    "\n",
    "            # Rename the extracted file\n",
    "            os.rename(\"Customer Churn.csv\", extracted_file)\n",
    "        else:\n",
    "            print(f\"File '{extracted_file}' already extracted. Skipping extraction.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'churn_data.zip' already exists. Skipping download.\n",
      "File 'churn_data.csv' already extracted. Skipping extraction.\n",
      "File 'winequality-red.csv' already exists. Skipping download.\n"
     ]
    }
   ],
   "execution_count": 143
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ySwkAKSM4iQA"
   },
   "source": [
    "#### Task 2.1: Dataset Loading\n",
    "\n",
    "The first task deals with the loading of the datasets.\n",
    "When training networks in `NumPy`, all data needs to be stored as NumPy arrays.\n",
    "The data should be split between input matrix $\\mathbf X \\in \\mathbb R^{D\\times N}$ and target matrix $\\mathbf T \\in \\mathbb R^{O\\times N}$.\n",
    "We need to **add a bias neuron to the input** so that we have $\\mathbf X \\in \\mathbb R^{(D+1)\\times N}$\n",
    "\n",
    "\n",
    "<!-- For the targets, we have to be more careful as there are differences w.r.t. the applied loss function.\n",
    "\n",
    "*   For binary classification, we need $\\mathbf T \\in \\mathbb R^{1\\times N}$.\n",
    "*   For categorical classification, we only need the class indexes $\\vec t = [t^{[1]}, \\ldots, t^{[N]}]$ to be in dimension $\\mathbb N^N$. -->\n",
    "\n",
    "\n",
    "Implement a function that returns both the input and the target data for a given dataset.\n",
    "\n",
    "Note:\n",
    "\n",
    "1. You can use `csv.reader()` to read the dataset, or rely on other methods such as `pandas`\n",
    "2. Please note that in the wine dataset CSV file, all values are separated by `;`, whereas in the churn dataset, they are separated by `,`\n",
    "3. For the wine dataset, convert 6 target values into a one-hot matrix so that the dimension is $\\mathbb R^{6\\times N}$. **Be aware that the target values in the wine dataset do not start at index 0.**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mnlXaDVVDMd2",
    "ExecuteTime": {
     "end_time": "2025-03-18T10:22:38.645693Z",
     "start_time": "2025-03-18T10:22:38.616568Z"
    }
   },
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "\n",
    "def dataset(dataset_file, delimiter):\n",
    "    # read dataset\n",
    "    data = pd.read_csv(dataset_file, delimiter=delimiter)\n",
    "\n",
    "    print (f\"Loaded dataset with {len(data)} samples\")\n",
    "\n",
    "    # Convert to NumPy array\n",
    "    data = numpy.array(data)\n",
    "\n",
    "    # Get the input (data samples)\n",
    "    X = data[:, :-1].astype(float)\n",
    "    # Insert bias neuron (add a column of ones) to the input\n",
    "    X = numpy.insert(X, 0, 1, axis=1)\n",
    "\n",
    "    if dataset_file == \"winequality-red.csv\":\n",
    "        # target is in the last column and needs to be converted into one-hot format [0, 1]\n",
    "        # Creates [N, 6] matrix, which we transpose at the end\n",
    "        T = pd.get_dummies(data[:, -1], dtype=float)\n",
    "\n",
    "        # Converts back to numpy array\n",
    "        T = numpy.array(T)\n",
    "    else:\n",
    "        # target is in the last column and needs to be of type float\n",
    "        T = data[:, -1].astype(float)\n",
    "\n",
    "        # Ensures shape is [N, 1] instead of [N, ]\n",
    "        T = T.reshape(T.shape[0], 1)\n",
    "\n",
    "    # IMPORTANT: After transposing first row in X is bias neuron!\n",
    "    return numpy.transpose(X), numpy.transpose(T)\n",
    "\n",
    "#TODO: Check target class distribution of both datasets\n",
    "X_churn, T_churn = dataset(\"churn_data.csv\", \",\")\n",
    "X_wine, T_wine = dataset(\"winequality-red.csv\", \";\")\n",
    "\n",
    "# TODO: Check proportion of 0 and 1 as target classes in churn dataset\n",
    "print(f\"Proportion of 0 as target class in T_churn: {T_churn[T_churn == 0.0].shape[0]/T_churn.shape[1]}\")\n",
    "print(f\"Proportion of 1 as target class in T_churn: {T_churn[T_churn == 1.0].shape[0]/T_churn.shape[1]}\")\n",
    "\n",
    "# TODO: Check proportion of each target class w.r.t to all observations in wine dataset\n",
    "sum_wine_target_class = numpy.sum(T_wine, axis=1, keepdims=True)\n",
    "proportion_wine_target_class = sum_wine_target_class / T_wine.shape[1]\n",
    "print(f\"Proportion of each target class in T_wine: {proportion_wine_target_class}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 3150 samples\n",
      "Loaded dataset with 1599 samples\n",
      "Proportion of 0 as target class in T_churn: 0.8428571428571429\n",
      "Proportion of 1 as target class in T_churn: 0.15714285714285714\n",
      "Proportion of each target class in T_wine: [[0.00625391]\n",
      " [0.03314572]\n",
      " [0.42589118]\n",
      " [0.39899937]\n",
      " [0.12445278]\n",
      " [0.01125704]]\n"
     ]
    }
   ],
   "execution_count": 144
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "15Su4txz4iQB"
   },
   "source": [
    "#### Test 1: Assert Valid Outputs\n",
    "\n",
    "\n",
    "1. For the churn data, we assure that all dimensions are correct and that class labels are in $\\{0, 1\\}$.\n",
    "\n",
    "2. For the wine dataset, we make sure that the dataset is in the correct dimensions, i.e., $\\mathbf X\\in \\mathbb R^{(D+1)\\times N}$ and $\\mathbf T \\in \\mathbb N^{O\\times N}$. We verify that each sample in $\\mathbf T$ has exactly one active class (one-hot encoding)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5JsZT_cmTgBA",
    "outputId": "608efddd-199f-4988-e685-5f3796c6eee4",
    "ExecuteTime": {
     "end_time": "2025-03-18T10:22:38.770299Z",
     "start_time": "2025-03-18T10:22:38.706565Z"
    }
   },
   "source": [
    "# Load datasets using the dataset function\n",
    "X, T = dataset(\"churn_data.csv\", \",\")\n",
    "\n",
    "print(f\"Churn shape X: {X.shape}, T shape: {T.shape}\")\n",
    "\n",
    "\n",
    "# Assert checks for the churn dataset\n",
    "assert X.shape[0] == 14, X.shape[0]\n",
    "assert T.shape[0] == 1, T.shape[0]\n",
    "assert numpy.all(T >= 0) and numpy.all(T <= 1)\n",
    "assert T.dtype == numpy.float64\n",
    "\n",
    "# Load wine dataset\n",
    "X, T = dataset(\"winequality-red.csv\", \";\")\n",
    "\n",
    "print(f\"Wine shape X: {X.shape}, T shape: {T.shape}\")\n",
    "\n",
    "# Assert checks for the winequality-red dataset\n",
    "assert X.shape[0] == 12, X.shape[0]\n",
    "assert T.shape[0] == 6, T.shape[0]\n",
    "# assert numpy.all(T >= 0) and numpy.all(T <= 1)\n",
    "assert numpy.all(numpy.sum(T, axis=0) == 1), \"Error: Some samples are not correctly one-hot encoded!\" # ensures that each column sums to exactly 1\n",
    "assert T.dtype == numpy.float64"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 3150 samples\n",
      "Churn shape X: (14, 3150), T shape: (1, 3150)\n",
      "Loaded dataset with 1599 samples\n",
      "Wine shape X: (12, 1599), T shape: (6, 1599)\n"
     ]
    }
   ],
   "execution_count": 145
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "bSHMBMNN4iQB"
   },
   "source": [
    "#### Task 2.2: Split Training and Validation Data\n",
    "\n",
    "\n",
    "The data should be split into 80% for training and 20% for validation. Implement a function that takes the full dataset $(X,T)$ and returns $(X_t, T_t, X_v, T_v)$ accordingly.\n",
    "\n",
    "Write a function that splits off training and validation samples from a given dataset. **What do we need to assure before splitting?**\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9bhG1l6CD4gm",
    "ExecuteTime": {
     "end_time": "2025-03-18T10:22:38.804318Z",
     "start_time": "2025-03-18T10:22:38.796484Z"
    }
   },
   "source": [
    "def split_training_data(X,T,train_percentage=0.8):\n",
    "  # TODO: We need to assure smth, e.g. shuffle dataset to minimize bias?\n",
    "  seed = 27\n",
    "  rng = numpy.random.default_rng(seed)\n",
    "\n",
    "  # Assuming X.shape[1] = T.shape[1]\n",
    "  shuffled_indices = rng.permutation(X.shape[1])\n",
    "\n",
    "  # Shuffle both X and T in consistent manner\n",
    "  X_shuffled = X[:, shuffled_indices]\n",
    "  T_shuffled = T[:, shuffled_indices]\n",
    "\n",
    "  # split into 80/20 training/validation\n",
    "  X_train = X_shuffled[:, :int(X.shape[1] * train_percentage)]\n",
    "  T_train = T_shuffled[:, :int(T.shape[1] * train_percentage)]\n",
    "  X_val = X_shuffled[:, int(X.shape[1] * train_percentage):]\n",
    "  T_val = T_shuffled[:, int(T.shape[1] * train_percentage):]\n",
    "\n",
    "  return X_train, T_train, X_val, T_val"
   ],
   "outputs": [],
   "execution_count": 146
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Sk6_LL4X4iQC"
   },
   "source": [
    "#### Task 2.3: Input Data Normalization\n",
    "\n",
    "\n",
    "Since the data is in different input regimes, we want to normalize the data.\n",
    "\n",
    "Implement a function that normalizes all input data using the whitening method with given mean and standard deviation.\n",
    "$$X_{norm} = \\frac{X-\\mu}{\\sigma}$$\n",
    "Compute the mean ($\\mu$) and the standard deviation ($\\sigma$) for your dataset.\n",
    "Make sure that you handle the bias neuron $x_0$ correctly.\n",
    "Finally, normalize your input data using the implemented function.\n",
    "\n",
    "Note: Use `numpy.mean()` and `numpy.std()` with the correct dimensions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Eizcv6sqjpP0",
    "ExecuteTime": {
     "end_time": "2025-03-18T10:22:38.849016Z",
     "start_time": "2025-03-18T10:22:38.839485Z"
    }
   },
   "source": [
    "def normalize(X_train, X_val):\n",
    " # Compute mean and standard deviation for each feature (row)\n",
    "  # Keep shape with the keepdims parameter\n",
    "  mean = lambda x: numpy.mean(x, axis=1, keepdims=True)\n",
    "  std = lambda x: numpy.std(x, axis=1, keepdims=True)\n",
    "\n",
    "  # Separate the bias, first row of train and validate datasets\n",
    "  X_train_bias = X_train[0, :]\n",
    "  X_val_bias = X_val[0, :]\n",
    "\n",
    "  # All other rows of train and validate datasets\n",
    "  X_train_without_bias= X_train[1:, :]\n",
    "  X_val_without_bias = X_val[1:, :]\n",
    "\n",
    "  # Standardize both X_train and X_val\n",
    "  X_train_normalize_without_bias = ((X_train_without_bias - mean(X_train_without_bias)) / std(X_train_without_bias))\n",
    "  X_val_normalize_without_bias = ((X_val_without_bias - mean(X_val_without_bias)) / std(X_val_without_bias))\n",
    "\n",
    "  # Add the bias row back\n",
    "  X_train_normalize = numpy.vstack((X_train_bias, X_train_normalize_without_bias))\n",
    "  X_val_normalize = numpy.vstack((X_val_bias, X_val_normalize_without_bias))\n",
    "\n",
    "  return X_train_normalize, X_val_normalize\n",
    "\n",
    "# TODO: For testing purposes only\n",
    "# X, T = dataset(\"churn_data.csv\", \",\")\n",
    "# X_train, T_train, X_val, T_val = split_training_data(X, T)\n",
    "#\n",
    "# X_train_df = pd.DataFrame(X_train)\n",
    "# X_val_df = pd.DataFrame(X_val)\n",
    "# print(f\"X Train Dataset: {X_train_df.head()}\")\n",
    "# print(f\"X Val Dataset: {X_val_df.head()}\")\n",
    "#\n",
    "# X_train_normalize, X_val_normalize = normalize(X_train, X_val)\n",
    "# print(f\"X Train Normalize Dataset: {pd.DataFrame(X_train_normalize).head()}\")\n",
    "# print(f\"X Val Normalize Dataset: {pd.DataFrame(X_val_normalize).head()}\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 147
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "9cTqq3UnknyP"
   },
   "source": [
    "### Gradient Descent\n",
    "\n",
    "To train a two-layer multi-output regression network, we need to implement some functions.\n",
    "The network output is computed in three steps:\n",
    "\n",
    "  * Compute network activation for a batch of inputs $\\mathbf X$: $\\mathbf A = \\mathbf W^{(1)}\\mathbf X$\n",
    "  * Call the activation function element-wise: $\\mathbf H = g(\\mathbf A)$. Here, we rely on the $\\tanh$ function. Assure that the hidden neuron bias $\\mathbf H_{0,:}$ is set appropriately.\n",
    "  * Compute the logits $\\mathbf Z$ of the batch: $\\mathbf Z = \\mathbf W^{(2)}\\mathbf H$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Sl3UYyPmXy_2"
   },
   "source": [
    "#### Task 2.4: Activation of Output\n",
    "\n",
    "In this task, we will implement the activation functions used in binary and categorical classification:  \n",
    "\n",
    "1. Logistic Function (for Binary Classification)  \n",
    "   $$\n",
    "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "   $$\n",
    "\n",
    "2. Stable Softmax Function (for Categorical Classification)\n",
    "  $$\n",
    "   \\tilde{\\mathcal S}(z_o) = \\frac{e^{z_o - z_{max}}}{\\sum\\limits_{o'} e^{z_{o'} - z_{max}}}\n",
    "   $$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jpcuXTBKg115",
    "ExecuteTime": {
     "end_time": "2025-03-18T10:22:38.899343Z",
     "start_time": "2025-03-18T10:22:38.882740Z"
    }
   },
   "source": [
    "def logistic_function(z):\n",
    "    return 1 / (1 + numpy.exp(-z))\n",
    "\n",
    "def stable_softmax(z): \n",
    "    # Compute nominator\n",
    "    e_z = numpy.exp(z - numpy.max(z))\n",
    "\n",
    "    # Compute denominator\n",
    "    z_diff = z - numpy.max(z)\n",
    "    e_sum = numpy.exp(z_diff)\n",
    "    return e_z / e_sum"
   ],
   "outputs": [],
   "execution_count": 148
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KMz-nSri4iQC"
   },
   "source": [
    "#### Task 2.5: Network Implementation\n",
    "\n",
    "Implement a multi-target network that computes the output matrix $\\mathbf Y$ for a given input dataset $\\mathbf X$ and given parameters $\\Theta=(\\mathbf W^{(1)}, \\mathbf W^{(2)})$ using `numpy` operations. Use $\\tanh$ as activation function between two layers.\n",
    "\n",
    "The function should return both the output $\\mathbf Y$ and the output of the hidden units $\\mathbf H$ since we will need these in gradient descent. \n",
    "When applying binary cross entropy loss, we apply the logistic function $\\sigma$ to the logits $\\mathbf Z$ before returning it.\n",
    "When applying categorical cross entropy loss, we apply stable softmax $\\tilde{\\mathcal S}$ to the logits $\\mathbf Z$ before returning it."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6H8ZV4Qcly95",
    "ExecuteTime": {
     "end_time": "2025-03-18T10:22:38.939756Z",
     "start_time": "2025-03-18T10:22:38.932581Z"
    }
   },
   "source": [
    "def Network(X, Theta, loss_type='bce'):\n",
    "  W1, W2 = Theta\n",
    "\n",
    "  print(f\"W1 shape: {W1.shape}\")\n",
    "  print(f\"W2 shape: {W2.shape}\")\n",
    "  print(f\"X shape: {X.shape}\")\n",
    "\n",
    "  # compute activation, A shape ?\n",
    "  A = numpy.dot(W1, X.T)\n",
    "\n",
    "  print(f\"A shape: {A.shape}\")\n",
    "\n",
    "  # compute hidden unit output\n",
    "  H = numpy.tanh(A)\n",
    "\n",
    "  # Prepend H0 as 1 to hidden unit output?\n",
    "  H_with_bias = numpy.concatenate((numpy.array([1.]), H))\n",
    "  \n",
    "  # Calculate Z\n",
    "  Z = numpy.dot(W2, H_with_bias)\n",
    "\n",
    "  # compute network output\n",
    "  if loss_type == 'bce':\n",
    "      # Apply logistic function to Z\n",
    "      Y = logistic_function(Z)\n",
    "      pass\n",
    "  else:\n",
    "      # Apply softmax stable function to Z\n",
    "      Y = stable_softmax(Z)\n",
    "      pass\n",
    "\n",
    "  # TODO: What should be the shape of the output, [number_of_observations, 1] / [1, number_of_observations]\n",
    "  return Y, H"
   ],
   "outputs": [],
   "execution_count": 149
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Zt0CMab4_Dr5"
   },
   "source": [
    "#### Task 2.6: Loss Implementation\n",
    "\n",
    "Implement a loss function that returns the Binary Cross-Entropy (BCE) loss for binary classification and the Categorical Cross-Entropy (CCE) loss for categorical classification.\n",
    "Note that for BCE, $O=1$ so that both $\\mathbf Y\\in\\mathbb R^{O\\times N}$ and $\\mathbf T\\in\\mathbb N^{O\\times N}$ are considered as vectors.\n",
    "\n",
    "$$\n",
    "\\mathcal J^{\\mathrm{BCE}} = -\\frac{1}{N} \\left[ \\mathbf T \\log \\mathbf Y + (1 - \\mathbf T) \\log (1 - \\mathbf Y) \\right] \\vec{\\mathbf 1}_N\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal J^{\\mathrm{CCE}} = -\\frac{1}{N} \\vec{\\mathbf 1}_O^T [\\mathbf T \\log \\mathbf Y] \\vec{\\mathbf 1}_N\n",
    "$$\n",
    "\n",
    "For both losses, the mathematical expressions below can be implemented much easier, by using basic `numpy` or `scipy` functionality."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MyudNNSLqRgq",
    "ExecuteTime": {
     "end_time": "2025-03-18T10:22:39.035547Z",
     "start_time": "2025-03-18T10:22:38.972431Z"
    }
   },
   "source": [
    "def loss(Y, T, loss_type):\n",
    "  # Assuming loss_type\n",
    "  if loss_type == 'bce':\n",
    "      return - numpy.sum(T * numpy.log(Y) + (1-T) * numpy.log(1-Y))\n",
    "  else:\n",
    "      return - numpy.sum(T * numpy.log(Y))"
   ],
   "outputs": [],
   "execution_count": 150
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "fD2srCKN_Dr5"
   },
   "source": [
    "#### Task 2.7: Gradient Implementation\n",
    "\n",
    "Implement a function that computes and returns the gradient for a given batch $(\\mathbf X, \\mathbf T)$, the given network outputs $\\mathbf Y$ and $\\mathbf H$ as well as current parameters $\\Theta=(\\mathbf W^{(1)}, \\mathbf W^{(2)})$.\n",
    "Make sure to compute the gradient with respect to both weight matrices. Remember that we have used $\\sigma$ as the activation function.\n",
    "Implement the function using the fast version provided in the lecture and make use of `numpy` operations."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nNpZRBrUqU5Q",
    "ExecuteTime": {
     "end_time": "2025-03-18T10:22:39.076381Z",
     "start_time": "2025-03-18T10:22:39.067493Z"
    }
   },
   "source": [
    "def gradient(X, T, Y, H, Theta):\n",
    "  W1, W2 = Theta\n",
    "\n",
    "  # Prepend H0 as 1 to hidden unit output\n",
    "  H_with_bias = numpy.concatenate((numpy.array([1.]), H))\n",
    "\n",
    "  g2 = numpy.dot((Y - T), H.T)\n",
    "\n",
    "  # Derivative with respect to H\n",
    "  dH = numpy.dot(W2.T, (Y - T))\n",
    "\n",
    "  # Derivative with respect to A\n",
    "  dA = dH * (1 - H ** 2)  # derivative of tanh\n",
    "\n",
    "  # Compute the derivative with respect to W1\n",
    "  g1 = numpy.dot(X.T, numpy.dot((Y - T), W2)) + numpy.dot(X.T, dA * H_with_bias)\n",
    "\n",
    "  # Compute the derivative with respect to W1\n",
    "  g1 = numpy.dot(X, dA * H_with_bias)\n",
    "\n",
    "  # Prepend H0 as 1 to hidden unit output\n",
    "  # Compute the derivative with respect to W1\n",
    "  g1 = numpy.dot(X.T, dA * H_with_bias)\n",
    "\n",
    "  return g1, g2\n"
   ],
   "outputs": [],
   "execution_count": 151
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "iQ5re_EW4iQD"
   },
   "source": [
    "#### Task 2.8: Accuracy Computation\n",
    "\n",
    "\n",
    "Implement a function that computes the accuracy of the provided network output and the given target values.\n",
    "Make sure that the implementation supports both binary as well as categorical targets."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eDOQJPo3FLX4",
    "ExecuteTime": {
     "end_time": "2025-03-18T10:22:39.116329Z",
     "start_time": "2025-03-18T10:22:39.109295Z"
    }
   },
   "source": [
    "def accuracy(Y, T):\n",
    "  # check if we have binary or categorical classification\n",
    "  # Calculate accuracy as TP / TP+FP\n",
    "  if T.shape[0] == 1:\n",
    "        # binary classification\n",
    "        return numpy.sum((Y >= 0.5) == T) / T.shape[1]\n",
    "  else:\n",
    "        # categorical classification\n",
    "        return numpy.sum(numpy.argmax(Y, axis=0) == numpy.argmax(T, axis=0)) / T.shape[1]"
   ],
   "outputs": [],
   "execution_count": 152
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "qXZnMJ6O4iQD"
   },
   "source": [
    "#### Test 2: Test Accuracy Function\n",
    "\n",
    "\n",
    "Design test data and according logit values with which you can test the correctness of your accuracy function.\n",
    "Make sure that the accuracy will compute the correct values.\n",
    "Test both binary and categorical accuracy."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "T6oj8z3a4iQD",
    "ExecuteTime": {
     "end_time": "2025-03-18T10:22:39.220565Z",
     "start_time": "2025-03-18T10:22:39.143348Z"
    }
   },
   "source": [
    "# first, test binary classification\n",
    "YY = numpy.ones((1,20)) * -5.\n",
    "YY[0][15:20] = 5\n",
    "assert(abs(accuracy(YY,numpy.zeros((1,20))) - 0.75) < 1e-8)\n",
    "assert(abs(accuracy(YY,numpy.ones((1,20))) - 0.25) < 1e-8)\n",
    "\n",
    "# now, test categorical classification with 4 classes\n",
    "YY = numpy.ones((4, 20)) * -5  # Shape (C, N) to match function expectations\n",
    "YY[0, 0:1] = 5\n",
    "YY[1, 1:4] = 5\n",
    "YY[2, 4:10] = 5\n",
    "YY[3, 10:20] = 5\n",
    "# Modify YY and T to be one-hot encoded\n",
    "T0 = numpy.eye(4)[numpy.zeros(20, dtype=int)].T\n",
    "T1 = numpy.eye(4)[numpy.ones(20, dtype=int)].T\n",
    "T2 = numpy.eye(4)[numpy.ones(20, dtype=int) * 2].T\n",
    "T3 = numpy.eye(4)[numpy.ones(20, dtype=int) * 3].T\n",
    "T4 = numpy.eye(4)[numpy.array([0, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])].T\n",
    "\n",
    "# Assertions with modified inputs\n",
    "assert abs(accuracy(YY, T0) - 0.05) < 1e-8\n",
    "assert abs(accuracy(YY, T1) - 0.15) < 1e-8\n",
    "assert abs(accuracy(YY, T2) - 0.3) < 1e-8\n",
    "assert abs(accuracy(YY, T3) - 0.5) < 1e-8\n",
    "assert abs(accuracy(YY, T4) - 1.0) < 1e-8"
   ],
   "outputs": [],
   "execution_count": 153
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "yBC2C6sy4iQD"
   },
   "source": [
    "#### Task 2.9: Training Loop\n",
    "\n",
    "\n",
    "Implement gradient descent for a given number of 10'000 epochs using given input data, initial parameters $\\Theta$, loss_type, number of epochs, as well as a learning rate of $\\eta=0.1$.\n",
    "\n",
    "Make use of the normalized dataset from Task 2.3, the network from Task 2.5, the loss from Task 2.6, the gradient from Task 2.7, and the accuracy function from Task 2.8.\n",
    "\n",
    "Make sure that you train on the training data only, and not on the validation data.\n",
    "In each loop, compute and store the training loss, training accuracy, validation loss and validation accuracy.\n",
    "At the end, return the lists of these values.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LOhA6w23qnaK",
    "ExecuteTime": {
     "end_time": "2025-03-18T10:22:39.262944Z",
     "start_time": "2025-03-18T10:22:39.252377Z"
    }
   },
   "source": [
    "def train(X_train, T_train, X_val, T_val, Theta, loss_type, learning_rate = 0.1, epochs = 10_000):\n",
    "  # collect weights\n",
    "  W1, W2 = Theta\n",
    "\n",
    "  # collect loss and accuracy values\n",
    "  train_loss, train_acc, val_loss, val_acc = [], [], [], []\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    # train on training set\n",
    "    # ... compute network output on training data\n",
    "    Y, H = Network(X_train, Theta, loss_type)\n",
    "\n",
    "    # ... compute loss from network output and target data\n",
    "    loss_value = loss(Y, T_train, loss_type)\n",
    "\n",
    "    # ... compute gradient and perform weight update\n",
    "    g1, g2 = gradient(X, T, Y, H, Theta)\n",
    "    W1 -= learning_rate * g1\n",
    "    W2 -= learning_rate * g2\n",
    "\n",
    "    # ... update weights\n",
    "    Theta = W1, W2\n",
    "\n",
    "    # ... remember loss\n",
    "    train_loss.append(loss_value)\n",
    "\n",
    "    # ... compute training set accuracy\n",
    "    train_acc.append(accuracy(Y, T))\n",
    "\n",
    "    # test on validation data\n",
    "    # ... compute network output on validation data\n",
    "    Y_val, H_val = Network(X_val, Theta, loss_type)\n",
    "\n",
    "    # ... compute loss from network output and target data\n",
    "    val_loss_value = loss(Y_val, T_val, loss_type)\n",
    "\n",
    "    # ... remember loss\n",
    "    val_loss.append(val_loss_value)\n",
    "\n",
    "    # ... compute validation set accuracy\n",
    "    val_acc.append(accuracy(Y_val, T_val))\n",
    "\n",
    "  # return the four lists of losses and accuracies\n",
    "  return train_loss, train_acc, val_loss, val_acc"
   ],
   "outputs": [],
   "execution_count": 154
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "tU_LUDM14iQD"
   },
   "source": [
    "#### Task 2.10: Plotting Function\n",
    "\n",
    "\n",
    "Implement a function that takes four lists containing the training loss, the training accuracy, the validation loss and the validation accuracy.\n",
    "Plot the two losses into one plot, and the two accuracies into another plot."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_fIVZsZvsryb",
    "ExecuteTime": {
     "end_time": "2025-03-18T10:22:39.308564Z",
     "start_time": "2025-03-18T10:22:39.299035Z"
    }
   },
   "source": [
    "from matplotlib import pyplot\n",
    "def plot(train_loss, train_acc, val_loss, val_acc):\n",
    "  pyplot.figure(figsize=(10,3))\n",
    "  ax = pyplot.subplot(121)\n",
    "  ax.plot(train_loss, \"g-\", label=\"Training set loss\")\n",
    "  ax.plot(val_loss, \"b-\", label=\"Validation set loss\")\n",
    "  ax.set_xlabel(\"Epoch\")\n",
    "  ax.legend()\n",
    "\n",
    "  ax = pyplot.subplot(122)\n",
    "  ax.plot(train_acc, \"g-\", label=\"Training set accuracy\")\n",
    "  ax.plot(val_acc, \"b-\", label=\"Validation set accuracy\")\n",
    "  ax.set_xlabel(\"Epoch\")\n",
    "  ax.legend()\n",
    "\n",
    "  ax = pyplot.subplot(123)\n",
    "  ax.plot(train_acc, \"g-\", label=\"Training set accuracy\")\n",
    "  ax.plot(val_acc, \"b-\", label=\"Validation set accuracy\")\n",
    "  ax.set_xlabel(\"Epoch\")\n",
    "  ax.legend()"
   ],
   "outputs": [],
   "execution_count": 155
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "I0A03pI84iQE"
   },
   "source": [
    "#### Task 2.11: Binary Classification\n",
    "\n",
    "\n",
    "1. Load the data for binary classification, using the ``\"churn_data.csv\"`` file.\n",
    "2. Split the data into training and validation sets.\n",
    "3. Normalize both training and validation input data using the function from Task 2.3.\n",
    "4. Instantiate the weight matrices $\\Theta=(\\mathbf W^{(1)}, \\mathbf W^{(2)})$ using the Xavier method as introduced in the lecture\n",
    "5. Train the network on the churn data with the learning rate of **$\\eta=0.1$** for 10'000 epochs and plot the training and validation accuracies and losses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6dQdkFKStw2g",
    "ExecuteTime": {
     "end_time": "2025-03-18T10:22:39.515385Z",
     "start_time": "2025-03-18T10:22:39.370441Z"
    }
   },
   "source": [
    "# load dataset\n",
    "X, T = dataset(\"churn_data.csv\", \",\")\n",
    "\n",
    "# split dataset\n",
    "X_train, T_train, X_val, T_val = split_training_data(X, T)\n",
    "\n",
    "# normalize input data\n",
    "X_train, X_val = (X_train - X_train.mean(axis=0)) / X_train.std(axis=0), (X_val - X_val.mean(axis=0)) / X_val.std(axis=0)\n",
    "\n",
    "K = 10\n",
    "D = X_train.shape[1]\n",
    "O = 1\n",
    "W1 = np.random.randn(K, D)\n",
    "W2 = np.random.randn(K, O)\n",
    "Theta = [W1, W2]\n",
    "\n",
    "# train network on our data\n",
    "train_loss, train_acc, val_loss, val_acc = train(X_train, T_train, X_val, T_val, Theta, loss_type=\"bce\")\n",
    "\n",
    "# plot the results\n",
    "plot(train_loss, train_acc, val_loss, val_acc)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 3150 samples\n",
      "W1 shape: (10, 2520)\n",
      "W2 shape: (10, 1)\n",
      "X shape: (14, 2520)\n",
      "A shape: (10, 14)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 1 has 2 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[156], line 18\u001B[0m\n\u001B[0;32m     15\u001B[0m Theta \u001B[38;5;241m=\u001B[39m [W1, W2]\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m# train network on our data\u001B[39;00m\n\u001B[1;32m---> 18\u001B[0m train_loss, train_acc, val_loss, val_acc \u001B[38;5;241m=\u001B[39m train(X_train, T_train, X_val, T_val, Theta, loss_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbce\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# plot the results\u001B[39;00m\n\u001B[0;32m     21\u001B[0m plot(train_loss, train_acc, val_loss, val_acc)\n",
      "Cell \u001B[1;32mIn[154], line 11\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(X_train, T_train, X_val, T_val, Theta, loss_type, learning_rate, epochs)\u001B[0m\n\u001B[0;32m      6\u001B[0m train_loss, train_acc, val_loss, val_acc \u001B[38;5;241m=\u001B[39m [], [], [], []\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[0;32m      9\u001B[0m   \u001B[38;5;66;03m# train on training set\u001B[39;00m\n\u001B[0;32m     10\u001B[0m   \u001B[38;5;66;03m# ... compute network output on training data\u001B[39;00m\n\u001B[1;32m---> 11\u001B[0m   Y, H \u001B[38;5;241m=\u001B[39m Network(X_train, Theta, loss_type)\n\u001B[0;32m     13\u001B[0m   \u001B[38;5;66;03m# ... compute loss from network output and target data\u001B[39;00m\n\u001B[0;32m     14\u001B[0m   loss_value \u001B[38;5;241m=\u001B[39m loss(Y, T_train, loss_type)\n",
      "Cell \u001B[1;32mIn[149], line 17\u001B[0m, in \u001B[0;36mNetwork\u001B[1;34m(X, Theta, loss_type)\u001B[0m\n\u001B[0;32m     14\u001B[0m H \u001B[38;5;241m=\u001B[39m numpy\u001B[38;5;241m.\u001B[39mtanh(A)\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# Prepend H0 as 1 to hidden unit output?\u001B[39;00m\n\u001B[1;32m---> 17\u001B[0m H_with_bias \u001B[38;5;241m=\u001B[39m numpy\u001B[38;5;241m.\u001B[39mconcatenate((numpy\u001B[38;5;241m.\u001B[39marray([\u001B[38;5;241m1.\u001B[39m]), H))\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# Calculate Z\u001B[39;00m\n\u001B[0;32m     20\u001B[0m Z \u001B[38;5;241m=\u001B[39m numpy\u001B[38;5;241m.\u001B[39mdot(W2, H_with_bias)\n",
      "\u001B[1;31mValueError\u001B[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 1 has 2 dimension(s)"
     ]
    }
   ],
   "execution_count": 156
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "UtB7EEI14iQE"
   },
   "source": [
    "#### Task 2.12: Categorical Classification\n",
    "\n",
    "\n",
    "1. Load the data for categorical classification, using the ``\"winequality-red.csv\"`` file.\n",
    "2. Split the data into training and validation sets.\n",
    "3. Normalize both training and validation input data using the function from Task 2.3.\n",
    "4. **How many input and output neurons do we need?** Change the number of input, hidden, and output neurons accordingly. Select an appropriate number of hidden neurons $K$.\n",
    "5. Instantiate the weight matrices $\\Theta=(\\mathbf W^{(1)}, \\mathbf W^{(2)})$ using the Xavier method as introduced in the lecture\n",
    "6. Train the network on the winequality-red dataset with the learning rate of **$\\eta=0.1$** for 10'000 epochs and plot the training and validation accuracies and losses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oBADNP7Muif5"
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "X, T = dataset(\"winequality-red.csv\", \";\")\n",
    "\n",
    "# split dataset\n",
    "X_train, T_train, X_val, T_val = split_training_data(X, T)\n",
    "\n",
    "# normalize input data\n",
    "X_train, X_val = (X_train - X_train.mean(axis=0)) / X_train.std(axis=0), (X_val - X_val.mean(axis=0)) / X_val.std(axis=0)\n",
    "\n",
    "K = 10\n",
    "D = X_train.shape[1]\n",
    "O = 1\n",
    "W1 = np.random.randn(K, D)\n",
    "W2 = np.random.randn(K, O)\n",
    "Theta = [W1, W2]\n",
    "\n",
    "# train network on our data\n",
    "train_loss, train_acc, val_loss, val_acc = train(X_train, T_train, X_val, T_val, Theta, loss_type=\"cce\")\n",
    "\n",
    "# plot the results\n",
    "plot(train_loss, train_acc, val_loss, val_acc)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "DL_FS2025_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
