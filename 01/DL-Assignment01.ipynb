{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Members:\n",
    "\n",
    "- Name, matriculation number\n",
    "- Name, matriculation number\n",
    "- Name, matriculation number"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ZmubxigEE-Zb"
   },
   "source": [
    "# Assignment 1: Universal Function Approximator\n",
    "\n",
    "\n",
    "The goal of this exercise is to compare three different neural network architectures and analyze their capacity for function approximation:\n",
    "\n",
    "1. $N_1$: One-layer network (linear transformation only)\n",
    "2. $N_2$: One-layer network with non-linear activation function\n",
    "3. $N_3$: Two-layer network (hidden layer with non-linear activation function)\n",
    "\n",
    "They will be trained via gradient descent (with weight decay). To show the flexibility of the approach, three different functions will be approximated:\n",
    "1. $X_1: t = \\cos(3x)$ for $x\\in[-2,2]$\n",
    "2. $X_2: t = e^{-x^2}$ for $x\\in[-1,1]$\n",
    "3. $X_3: t = x^5 + 3x^4 - 6x^3 -12x^2 + 5x + 129$ for $x\\in[-4,2.5]$\n",
    "\n",
    "In the theoretical section, the networks will be designed, and the necessary derivatives will be computed by hand.\n",
    "\n",
    "In the coding section, we will: \n",
    "\n",
    "- implement the networks and their gradients,\n",
    "- generate target data for three different functions, \n",
    "- apply the training procedure to the data, and \n",
    "- plot the resulting approximated function together with the data samples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "n36PMmPANmJ7"
   },
   "source": [
    "## Section 1: Theoretical Questions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "oXFApo7obLKe"
   },
   "source": [
    "### Network Design"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "FH8oL0-MQ0IY"
   },
   "source": [
    "#### Task 1.1: Network Structure\n",
    "\n",
    "Given input $\\vec x = (1, x)^T$, define three neural networks ($N_1$, $N_2$, $N_3$) mathematically, to reach output $y$. Use $g()$ to represent the activation function.\n",
    "\n",
    "Explain how their structures differ and analyze their function approximation capabilities.\n",
    "\n",
    "--- \n",
    "Note:\n",
    "\n",
    "For one-layer networks, define parameter $\\Theta=\\vec w \\in\\mathbb R^{D+1}$\n",
    "\n",
    "For two-layer network, define parameters $\\Theta=(\\mathbf W^{(1)},\\vec w^{(2)})$ that are split into $\\mathbf W^{(1)}\\in\\mathbb R^{K\\times {(D+1)}}$ for the first layer and $\\vec w^{(2)}\\in\\mathbb R^{K+1}$ for the second layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "kujWaTnHTEnk"
   },
   "source": [
    "$N_1$: ...\n",
    "\n",
    "$N_2$: ...\n",
    "\n",
    "$N_3$: ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "eSm46qR8SdQH"
   },
   "source": [
    "#### Task 1.2: Network Comparison\n",
    "\n",
    "Can the one-layer network approximate all three functions well? Why or why not?\n",
    "\n",
    "What advantages does the two-layer network have compared to a one-layer network?\n",
    "\n",
    "How can we determine the appropriate number of hidden neurons?\n",
    "When looking at the example plots in the OLAT, how many hidden neurons do we need in order to approximate the functions? Is there any difference between the three target functions?\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "N_ZK0aMfafSP"
   },
   "source": [
    "..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ki1cAn6zSvj2"
   },
   "source": [
    "#### Task 1.3: Network Performance\n",
    "\n",
    "If the network struggles to approximate a function well, what are some possible reasons?\n",
    "\n",
    "How can we improve the network's performance?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "DP60Na-kbF_1"
   },
   "source": [
    "##### Reasons\n",
    "...\n",
    "\n",
    "##### Solutions\n",
    "..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "UPUvWGhybtv9"
   },
   "source": [
    "### Derivatives"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "F9ibD4zrPOvE"
   },
   "source": [
    "#### Task 1.4: Activation Function\n",
    "\n",
    "Given the hyperbolic tangent ($\\tanh$) activation function as:\n",
    "\n",
    "$$\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "\n",
    "Prove:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial x} \\tanh(x) = 1 - \\tanh^2(x)$$\n",
    "\n",
    "Hint: Apply the derivative rules as defined in the Lecture:\n",
    "* Quotient rule\n",
    "* Sum rule\n",
    "* Exponential rule\n",
    "\n",
    "Also, avoid factoring out parentheses."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "TW6QrQKPUffH"
   },
   "source": [
    "$\\frac{\\partial}{\\partial x} \\tanh(x) =...$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "mC041GO7PzIP"
   },
   "source": [
    "#### Task 1.5: Weight Decay\n",
    "\n",
    "Consider a loss function with L2 regularization (weight decay):\n",
    "$$\n",
    "L'(\\theta) = L(\\theta) + \\frac{\\lambda}{2} \\|\\theta\\|^2\n",
    "$$\n",
    "\n",
    "Compute its derivative with respect to $\\theta$: $$\\frac{\\partial}{\\partial \\theta} L'(\\theta)$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "2o4IjaZ4VimR"
   },
   "source": [
    "$\\frac{\\partial}{\\partial \\theta} L'(\\theta) = ...$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "aOiw6bNJQG5E"
   },
   "source": [
    "#### Task 1.6\n",
    "\n",
    "How large should an appropriate weight decay parameter $\\lambda$ as shown in Task 1.5 be? What would happen if $\\lambda$ is set too high or too low?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "vErgVANhV9cu"
   },
   "source": [
    "..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "SY8F0hc3Ttyn"
   },
   "source": [
    "## Section 2: Coding\n",
    "\n",
    "**<font color='red' size='5'>This section has to be submitted by 11:59 p.m. on Wednesday, March 12th, to be graded.</font>**\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "pLvRiQ-6NRB8"
   },
   "source": [
    "### Network Implementation\n",
    "#### Task 2.1\n",
    "\n",
    "Recall that for one-layer networks, we define parameter $\\Theta=\\vec w \\in\\mathbb R^{D+1}$, and for a two-layer network, we define parameters $\\Theta=(\\mathbf W^{(1)}\\in\\mathbb R^{K\\times {(D+1)}},\\vec w^{(2)}\\in\\mathbb R^{K+1})$.\n",
    "\n",
    "- D: The dimension of the input. In this assignment, $D = 1$ since there is only one input.\n",
    "- K: The number of hidden neurons in the first layer of the two-layer network ($N_3$)\n",
    "\n",
    "Implement a function that returns the network output for a given input $\\vec x$, parameter(s) $\\Theta$, and model_type ($N_1$, $N_2$, or $N_3$). Remember that the input of the function $\\vec x = (1, x)^T$.\n",
    "\n",
    "---\n",
    "Note:\n",
    "\n",
    "1. Use the `numpy` to implement the $\\tanh$ function.\n",
    "2. Use `numpy.concatenate` or `numpy.insert` to prepend $h_0$.\n",
    "3. Make use of `numpy.dot` to compute matrix-vector and vector-vector products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "4XOSElIwJ5BB"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def network(x, Theta, model_type):\n",
    "    \"\"\"\n",
    "    Compute the output of a neural network model.\n",
    "\n",
    "    Args:\n",
    "        x: Input vector (1, x) including bias.\n",
    "        Theta: Tuple of network parameters (W1, w2).\n",
    "        model_type: Type of model (1, 2, or 3).\n",
    "\n",
    "    Returns:\n",
    "        y: Network output.\n",
    "        h: Hidden layer output, or None.\n",
    "    \"\"\"\n",
    "\n",
    "    W1, w2 = Theta # w2 is None if model_type is 1 or 2\n",
    "\n",
    "    if model_type == 1:\n",
    "        # One-layer network (Linear Model)\n",
    "        y = numpy.dot(x, W1)\n",
    "        return y, None # To make this consistent when model_type is 3\n",
    "\n",
    "    elif model_type == 2:\n",
    "        # One-layer network with tanh activation\n",
    "        y = numpy.tanh(numpy.dot(x, W1))\n",
    "        y = numpy.tanh(numpy.dot(x, W1))\n",
    "        return y, None\n",
    "\n",
    "    elif model_type == 3:\n",
    "        # Two-layer network with tanh activation\n",
    "        a_ = numpy.dot(W1, x)  # W1 shape: (K, D+1); x shape: (D+1,)\n",
    "        h_ = numpy.tanh(a_)\n",
    "        h = numpy.concatenate((numpy.array([1.]), h_))  # prepend bias term; h shape becomes (K+1,)\n",
    "        y = numpy.dot(h, w2)  # w2 shape should be (K+1,)\n",
    "        return y, h\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "g5XL_ohAE-Zh"
   },
   "source": [
    "#### Test 1: Sanity Check\n",
    "\n",
    "We select a specific number of hidden neurons and create the weights accordingly, using all zeros in the first layer and all ones in the second. The test case below ensures that the function from Task 1 actually returns $11$ for those weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ku3Sy5fzj8YH",
    "outputId": "45b07ac7-686d-432e-8d09-1342af087687"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N1 test passed.\n",
      "N2 test passed.\n",
      "N3 test passed.\n"
     ]
    }
   ],
   "source": [
    "# Define test parameters\n",
    "K = 20\n",
    "D = 1\n",
    "Theta_one_layer = [numpy.ones(D+1),None]\n",
    "Theta_two_layer = [numpy.zeros((K, D+1)), numpy.ones(K+1)]\n",
    "x = numpy.random.rand(D+1)\n",
    "\n",
    "# Sanity check for N1\n",
    "y1, _ = network(x, Theta_one_layer, 1)\n",
    "assert abs(numpy.sum(x) - y1) < 1e-6\n",
    "print(\"N1 test passed.\")\n",
    "\n",
    "# Sanity check for N2\n",
    "y2, _ = network(x, Theta_one_layer, 2)\n",
    "assert abs(numpy.tanh(numpy.sum(x)) - y2) < 1e-6\n",
    "print(\"N2 test passed.\")\n",
    "\n",
    "# Sanity check for N3\n",
    "y3, _ = network(x, Theta_two_layer, 3)\n",
    "assert abs(1.0 - y3) < 1e-6\n",
    "print(\"N3 test passed.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "-fW7_KzcE-Zi"
   },
   "source": [
    "### Gradient Implementation\n",
    "#### Task 2.2: Gradient Computation\n",
    "\n",
    "\n",
    "Implementation of a function that returns the gradient as defined for a given dataset $X=\\{(\\vec x^{[n]}, t^{[n]})\\}$, given weight(s) $\\Theta$, model_type ($N_1$, $N_2$, or $N_3$), and $\\lambda$ parameter for weight decay.\n",
    "\n",
    "---\n",
    "Note:\n",
    "\n",
    "We should make sure that both parts of the gradient are computed for $N_3$ (since $\\Theta=(\\mathbf W^{(1)},\\vec w^{(2)})$ here).\n",
    "\n",
    "This is a very slow implementation. We will see how to speed this up in the next lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Vl_dYxcW-VD"
   },
   "outputs": [],
   "source": [
    "def compute_gradient(X, Theta, model_type, lambda_=1.):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the loss function with respect to the weights for each model type.\n",
    "\n",
    "    Args:\n",
    "        X: Dataset containing input-target pairs (x, t).\n",
    "        Theta: Network parameters (W1, w2).\n",
    "        model_type: Type of model (1, 2, or 3).\n",
    "        lambda_: Weight decay parameter. Default is 1.0.\n",
    "\n",
    "    Returns:\n",
    "        Gradients with respect to W1 and w2. For model_type 1 and 2, w2 is None.\n",
    "    \"\"\"\n",
    "\n",
    "    # split parameters for easier handling\n",
    "    W1, w2 = Theta # w2 is None if model_type is 1 or 2\n",
    "\n",
    "    # define gradient with respect to both parameters\n",
    "    dW1 = ... \n",
    "    dw2 = ... # dw2 is None if model_type is 1 or 2\n",
    "\n",
    "    # iterate over dataset\n",
    "    for x, t in X:\n",
    "        # compute the gradient\n",
    "        ...\n",
    "\n",
    "    # Add penalty term/weight decay\n",
    "    ...\n",
    "\n",
    "    # anything else?\n",
    "    ...\n",
    "\n",
    "    return dW1, dw2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "FqAMeoy5E-aG"
   },
   "source": [
    "#### Task 2.3: Gradient Descent\n",
    "\n",
    "The procedure of gradient descent is the repeated application of two steps.\n",
    "\n",
    "1. The gradient of loss $\\nabla_{\\Theta}\\mathcal J^{L_2}$ is computed based on the current value of the parameters $\\Theta$.\n",
    "2. The weights are updated by moving a small step in the direction of the negative gradient:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\Theta = \\Theta - \\eta \\nabla_{\\Theta}\\mathcal J\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "As a stopping criterion, we select the number of training epochs to be 10000.\n",
    "\n",
    "Implementation of a function that performs gradient descent for a given dataset $X$, given initial parameters $\\Theta$, a given learning rate $\\eta$, model_type ($N_1$, $N_2$, or $N_3$), and $\\lambda$ parameter for weight decay, and returns the optimized parameters $\\Theta^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hx6Jjs2e2CFX"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X, Theta, eta, model_type, lambda_=1.):\n",
    "    epochs = 10000\n",
    "\n",
    "    # perform iterative gradient descent\n",
    "    for _ in range(epochs):\n",
    "        # compute the gradient\n",
    "        ...\n",
    "\n",
    "        # update the parameters\n",
    "        ...\n",
    "\n",
    "    # return optimized parameters\n",
    "    return ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5gEeh7N8E-aH"
   },
   "source": [
    "### Datasets\n",
    "\n",
    "#### Task 2.4: Data Samples\n",
    "\n",
    "In total, we will test our gradient descent function with three different datasets. Particularly, we approximate\n",
    "\n",
    "1. $X_1: t = \\cos(3x)$ for $x\\in[-2,2]$\n",
    "2. $X_2: t = e^{-x^2}$ for $x\\in[-1,1]$\n",
    "3. $X_3: t = x^5 + 3x^4 - 6x^3 -12x^2 + 5x + 129$ for $x\\in[-4,2.5]$\n",
    "\n",
    "Generate dataset $X_1$,  for $N=60$ samples randomly drawn from range $x\\in[-2,2]$. Generate data $X_2$ for $N=50$ samples randomly drawn from range $x\\in[-1,1]$. Generate dataset $X_3$ for $N=200$ samples randomly drawn from range $x\\in[-4,2.5]$. Implement all three datasets as lists of tuples: $\\{(\\vec x^{[n]}, t^{[n]})\\mid 1\\leq n\\leq N\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1DdPBymdcNXx"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "negative dimensions are not allowed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m X1 = [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnumpy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m60\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m]\n\u001b[32m      2\u001b[39m X2 = [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m numpy.random.rand(\u001b[32m50\u001b[39m, \u001b[32m2\u001b[39m)]\n\u001b[32m      3\u001b[39m X3 = [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m numpy.random.rand(\u001b[32m200\u001b[39m, \u001b[32m2\u001b[39m)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mnumpy\\\\random\\\\mtrand.pyx:1238\u001b[39m, in \u001b[36mnumpy.random.mtrand.RandomState.rand\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mnumpy\\\\random\\\\mtrand.pyx:441\u001b[39m, in \u001b[36mnumpy.random.mtrand.RandomState.random_sample\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_common.pyx:312\u001b[39m, in \u001b[36mnumpy.random._common.double_fill\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: negative dimensions are not allowed"
     ]
    }
   ],
   "source": [
    "X1 = [x for x in numpy.random.rand(60, 2)]\n",
    "X2 = [x for x in numpy.random.rand(50, 2)]\n",
    "X3 = [x for x in numpy.random.rand(200, 2)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "6tGZqaiUE-aH"
   },
   "source": [
    "#### Test 2: Sanity Check\n",
    "\n",
    "The test case below ensures that the elements of each generated dataset are tuples with two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WrneyBJLE-aI",
    "outputId": "96be9dcb-0709-45d3-b209-5a226b429045"
   },
   "outputs": [],
   "source": [
    "assert all(\n",
    "    isinstance(x, (tuple,list)) and\n",
    "    len(x) == 2 and\n",
    "    isinstance(x[0], (tuple,list,numpy.ndarray)) and\n",
    "    len(x[0] == 2) and\n",
    "    isinstance(x[1], float)\n",
    "    for X in (X1, X2, X3)\n",
    "    for x in X\n",
    ")\n",
    "\n",
    "print('Test passed!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "v0p9mC4YE-aI"
   },
   "source": [
    "### Function Approximation\n",
    "Finally, we want to make use of our gradient descent implementation to approximate our functions. In order to see our success, we want to plot the functions together with the data.\n",
    "\n",
    "#### Task 2.5: Define hidden Neurons\n",
    "How many hidden neurons will we need for $N_3$? Use the answers from Task 1.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OLichgq7cNXy"
   },
   "outputs": [],
   "source": [
    "# Define the number of neurons for each target function based on your discussion\n",
    "K1 = ...\n",
    "K2 = ...\n",
    "K3 = ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "1fvx31eyE-aI"
   },
   "source": [
    "#### Task 2.6: Random Parameters\n",
    "\n",
    "For each of the networks, randomly initialize the parameters $\\Theta_1,\\Theta_2,\\Theta_3\\in[-1,1]$ for each of the datasets.\n",
    "\n",
    "For $N_3$, use the number of hidden neurons estimated in Task 1.2 and implemented in Task 2.5.\n",
    "\n",
    "---\n",
    "Note:\n",
    "\n",
    "  1. You can use `numpy.random.uniform` to initialize the weights.\n",
    "  2. Make sure that the weight matrices are instantiated in the correct dimensions.\n",
    "  3. Theta should always have two elements. The second element can be `None` for one-layer networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aq768_chcNXy"
   },
   "outputs": [],
   "source": [
    "Theta_N1 = {\n",
    "    \"X1\": ...,\n",
    "    \"X2\": ...,\n",
    "    \"X3\": ...\n",
    "}\n",
    "\n",
    "Theta_N2 = {\n",
    "    \"X1\": ...,\n",
    "    \"X2\": ...,\n",
    "    \"X3\": ...\n",
    "}\n",
    "\n",
    "Theta_N3 = {\n",
    "    \"X1\": ...,\n",
    "    \"X2\": ...,\n",
    "    \"X3\": ...\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "1jCM2nyRE-aI"
   },
   "source": [
    "#### Task 2.7: Run Gradient Descent\n",
    "\n",
    "For each network, call gradient descent function from Task 2.3 using the datasets $X_1, X_2, X_3$, the according created parameters $\\Theta_1,\\Theta_2,\\Theta_3$. Store the resulting optimized weights $\\Theta_1^*, \\Theta_2^*, \\Theta_3^*$.\n",
    "\n",
    "Based on your chosen learning rates $\\eta$ and weight decay parameter $\\lambda$, you may need to optimize them for these functions. Do you see any differences? What are the best learning rates that you can find?\n",
    "\n",
    "---\n",
    "<span style=\"color:red\">WARNING: Depending on the implementation, this might run for several minutes!</span>\n",
    "\n",
    "---\n",
    "Note:\n",
    "\n",
    "1. Start with $\\eta=0.1$ and play around with the learning rate improve adaptation.\n",
    "2. $\\eta=0.1$ is too large for $X_3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I4sjPUH_cNXz"
   },
   "outputs": [],
   "source": [
    "# N1\n",
    "# Call gradient descent function using the datasets and initial weights that you created above\n",
    "# Choose appropriate learning rates for each function\n",
    "...\n",
    "...\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uio-gjhi3GBx"
   },
   "outputs": [],
   "source": [
    "# N2\n",
    "# Call gradient descent function using the datasets and initial weights that you created above\n",
    "# Choose appropriate learning rates for each function\n",
    "...\n",
    "...\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dCPF-Q3C3GJk"
   },
   "outputs": [],
   "source": [
    "# N3\n",
    "# Call gradient descent function using the datasets and initial weights that you created above\n",
    "# Choose appropriate learning rates for each function\n",
    "...\n",
    "...\n",
    "..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8_C3TseE-aJ"
   },
   "source": [
    "### Data and Function Plotting\n",
    "\n",
    "### Task 2.8: Plotting Function\n",
    "\n",
    "Implement a plotting function that takes a given dataset $X$, given parameters $\\Theta$, model_type, and a defined range $R=[\\min,\\max]$. Each data sample $(x^{[n]},t^{[n]})$ of the dataset is plotted as an $''x''$. In order to plot the function that is approximated by the network, generate sufficient equally-spaced input values $x\\in R$, compute the network output $y$ for these inputs, and plot them with a line.\n",
    "\n",
    "---\n",
    "Note:\n",
    "\n",
    "  1. The dataset $X$ is defined as above, a list of tuples $(\\vec x, t)$.\n",
    "  2. Each input in the dataset is defined as $\\vec x = (1,x)^T$.\n",
    "  3. Equidistant points can be obtained via `numpy.arange`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6sII26VJcNXz"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "def plot(X, Theta, model_type, R):\n",
    "  # first, plot data samples\n",
    "  pyplot.plot(..., ..., \"rx\", label=\"Data\")\n",
    "\n",
    "  # define equidistant points from min (R[0]) to max (R[1]) to evaluate the network\n",
    "  x = ...\n",
    "  # compute the network outputs for these values\n",
    "  y = ...\n",
    "  # plot network approximation\n",
    "  pyplot.plot(x,y,\"k-\", label=\"network\")\n",
    "  pyplot.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "YXcO5e-sE-aJ"
   },
   "source": [
    "#### Task 2.9: Plot Three Functions\n",
    "\n",
    "For each of the datasets and their optimized parameters, call the plotting function from Task 2.8. Use range $R=[-3,3]$ for dataset $X_1$, range $R=[-2,2]$ for $X_2$, and range $R=[-5,4]$ for dataset $X_3$.\n",
    "\n",
    "Note that the first element of range $R$ should be the lowest $x$-location, and the second element of $R$ is the highest value for $x$.\n",
    "\n",
    "Repeat for three networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CY3YGkXgcNXz"
   },
   "outputs": [],
   "source": [
    "figure = pyplot.figure(figsize=(10,3))\n",
    "\n",
    "# plot first function\n",
    "pyplot.subplot(131)\n",
    "...\n",
    "\n",
    "# plot second function\n",
    "pyplot.subplot(132)\n",
    "...\n",
    "\n",
    "# plot third function\n",
    "pyplot.subplot(133)\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
