{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Members:\n",
    "\n",
    "- Name, matriculation number\n",
    "- Name, matriculation number\n",
    "- Name, matriculation number"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ZmubxigEE-Zb"
   },
   "source": [
    "# Assignment 1: Universal Function Approximator\n",
    "\n",
    "\n",
    "The goal of this exercise is to compare three different neural network architectures and analyze their capacity for function approximation:\n",
    "\n",
    "1. $N_1$: One-layer network (linear transformation only)\n",
    "2. $N_2$: One-layer network with non-linear activation function\n",
    "3. $N_3$: Two-layer network (hidden layer with non-linear activation function)\n",
    "\n",
    "They will be trained via gradient descent (with weight decay). To show the flexibility of the approach, three different functions will be approximated:\n",
    "1. $X_1: t = \\cos(3x)$ for $x\\in[-2,2]$\n",
    "2. $X_2: t = e^{-x^2}$ for $x\\in[-1,1]$\n",
    "3. $X_3: t = x^5 + 3x^4 - 6x^3 -12x^2 + 5x + 129$ for $x\\in[-4,2.5]$\n",
    "\n",
    "In the theoretical section, the networks will be designed, and the necessary derivatives will be computed by hand.\n",
    "\n",
    "In the coding section, we will: \n",
    "\n",
    "- implement the networks and their gradients,\n",
    "- generate target data for three different functions, \n",
    "- apply the training procedure to the data, and \n",
    "- plot the resulting approximated function together with the data samples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "n36PMmPANmJ7"
   },
   "source": [
    "## Section 1: Theoretical Questions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "oXFApo7obLKe"
   },
   "source": [
    "### Network Design"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "FH8oL0-MQ0IY"
   },
   "source": [
    "#### Task 1.1: Network Structure\n",
    "\n",
    "Given input $\\vec x = (1, x)^T$, define three neural networks ($N_1$, $N_2$, $N_3$) mathematically, to reach output $y$. Use $g()$ to represent the activation function.\n",
    "\n",
    "Explain how their structures differ and analyze their function approximation capabilities.\n",
    "\n",
    "--- \n",
    "Note:\n",
    "\n",
    "For one-layer networks, define parameter $\\Theta=\\vec w \\in\\mathbb R^{D+1}$\n",
    "\n",
    "For two-layer network, define parameters $\\Theta=(\\mathbf W^{(1)},\\vec w^{(2)})$ that are split into $\\mathbf W^{(1)}\\in\\mathbb R^{K\\times {(D+1)}}$ for the first layer and $\\vec w^{(2)}\\in\\mathbb R^{K+1}$ for the second layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "kujWaTnHTEnk"
   },
   "source": [
    "$N_1$: Linear network (Linear regression), $y = \\sum_{i=0}^{D+1}w_{i}*x_{i}$\n",
    "Drawback: Underfitting if data is not following a linear trend\n",
    "Advantage: Simple\n",
    "\n",
    "$N_2$: Single-layer network with sigmoid/hyperbolic activation function, $a=\\sum_{i=0}^{D+1}$ and $y=g(a)=\\frac{1}{1+e^{-a}}$ or $y=g(a)=\\frac{e^a-e^{-a}}{e^a+e^{-a}}$\n",
    "Drawback: Only one sigmoid / hyperbolic, again underfitting\n",
    "Advantage: Could estimate more precisely non-linear trends\n",
    "\n",
    "$N_3$: Two-layer network with a hidden sigmoid/hyperbolic activation function, $\\mathbf{a} = f(x) = W \\cdot x^{\\rightarrow}$, then activation function $\\mathbf{h}=g(\\mathbf{a})$, where g() is the sigmoid or hyperbolic activation function.\n",
    "Then add hidden bias.\n",
    "Lastly, calculate output: $y=f(\\mathbf{h})=\\mathbf{w}*\\mathbf{h}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "eSm46qR8SdQH"
   },
   "source": [
    "#### Task 1.2: Network Comparison\n",
    "\n",
    "Can the one-layer network approximate all three functions well? Why or why not?\n",
    "\n",
    "What advantages does the two-layer network have compared to a one-layer network?\n",
    "\n",
    "How can we determine the appropriate number of hidden neurons?\n",
    "When looking at the example plots in the OLAT, how many hidden neurons do we need in order to approximate the functions? Is there any difference between the three target functions?\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "N_ZK0aMfafSP"
   },
   "source": [
    "X1: ~ 4 neurons\n",
    "X2: ~ 2 neurons\n",
    "X3: ~ 5 neurons\n",
    "\n",
    "Remark: In lectures it was mentioned, that it might be exponential in D?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ki1cAn6zSvj2"
   },
   "source": [
    "#### Task 1.3: Network Performance\n",
    "\n",
    "If the network struggles to approximate a function well, what are some possible reasons?\n",
    "\n",
    "How can we improve the network's performance?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "DP60Na-kbF_1"
   },
   "source": [
    "##### Reasons\n",
    "...\n",
    "\n",
    "##### Solutions\n",
    "..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "UPUvWGhybtv9"
   },
   "source": [
    "### Derivatives"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "F9ibD4zrPOvE"
   },
   "source": [
    "#### Task 1.4: Activation Function\n",
    "\n",
    "Given the hyperbolic tangent ($\\tanh$) activation function as:\n",
    "\n",
    "$$\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "\n",
    "Prove:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial x} \\tanh(x) = 1 - \\tanh^2(x)$$\n",
    "\n",
    "Hint: Apply the derivative rules as defined in the Lecture:\n",
    "* Quotient rule\n",
    "* Sum rule\n",
    "* Exponential rule\n",
    "\n",
    "Also, avoid factoring out parentheses."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "TW6QrQKPUffH"
   },
   "source": [
    "$\\frac{\\partial}{\\partial x} \\tanh(x) =\\frac{\\frac{\\partial (e^x-e^{-x})}{\\partial x}*(e^x+e^{-x})-\\frac{\\partial (e^x+e^{-x})}{\\partial x}*(e^x-e^{-x})}{(e^x+e^{-x})^2}$\n",
    "\n",
    "$\\frac{\\partial}{\\partial x} \\tanh(x) =\\frac{(e^x+e^{-x})^2-(e^x-e^{-x})^2}{(e^x+e^{-x})^2}$\n",
    "\n",
    "$\\frac{\\partial}{\\partial x} \\tanh(x)=1- (\\frac{e^x-e^{-x}}{e^x+e^{-x}})^2$\n",
    "\n",
    "$\\frac{\\partial}{\\partial x} \\tanh(x) = 1 - tanh(x)^2$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "mC041GO7PzIP"
   },
   "source": [
    "#### Task 1.5: Weight Decay\n",
    "\n",
    "Consider a loss function with L2 regularization (weight decay):\n",
    "$$\n",
    "L'(\\theta) = L(\\theta) + \\frac{\\lambda}{2} \\|\\theta\\|^2\n",
    "$$\n",
    "\n",
    "Compute its derivative with respect to $\\theta$: $$\\frac{\\partial}{\\partial \\theta} L'(\\theta)$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "2o4IjaZ4VimR"
   },
   "source": [
    "$\\frac{\\partial}{\\partial \\theta} L'(\\theta) = \\frac{\\partial L(\\theta)}{\\partial \\theta} + \\frac{\\lambda}{2}*2*\\theta$\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta} L'(\\theta) = \\frac{\\partial L(\\theta)}{\\partial \\theta} + \\lambda*\\theta$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "aOiw6bNJQG5E"
   },
   "source": [
    "#### Task 1.6\n",
    "\n",
    "How large should an appropriate weight decay parameter $\\lambda$ as shown in Task 1.5 be? What would happen if $\\lambda$ is set too high or too low?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "vErgVANhV9cu"
   },
   "source": [
    "Too high: Risk of underfitting ?\n",
    "Too low: Risk of overfitting ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "SY8F0hc3Ttyn"
   },
   "source": [
    "## Section 2: Coding\n",
    "\n",
    "**<font color='red' size='5'>This section has to be submitted by 11:59 p.m. on Wednesday, March 12th, to be graded.</font>**\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "pLvRiQ-6NRB8"
   },
   "source": [
    "### Network Implementation\n",
    "#### Task 2.1\n",
    "\n",
    "Recall that for one-layer networks, we define parameter $\\Theta=\\vec w \\in\\mathbb R^{D+1}$, and for a two-layer network, we define parameters $\\Theta=(\\mathbf W^{(1)}\\in\\mathbb R^{K\\times {(D+1)}},\\vec w^{(2)}\\in\\mathbb R^{K+1})$.\n",
    "\n",
    "- D: The dimension of the input. In this assignment, $D = 1$ since there is only one input.\n",
    "- K: The number of hidden neurons in the first layer of the two-layer network ($N_3$)\n",
    "\n",
    "Implement a function that returns the network output for a given input $\\vec x$, parameter(s) $\\Theta$, and model_type ($N_1$, $N_2$, or $N_3$). Remember that the input of the function $\\vec x = (1, x)^T$.\n",
    "\n",
    "---\n",
    "Note:\n",
    "\n",
    "1. Use the `numpy` to implement the $\\tanh$ function.\n",
    "2. Use `numpy.concatenate` or `numpy.insert` to prepend $h_0$.\n",
    "3. Make use of `numpy.dot` to compute matrix-vector and vector-vector products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T17:57:05.208881Z",
     "start_time": "2025-03-10T17:57:05.200562Z"
    },
    "id": "4XOSElIwJ5BB"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def network(x, Theta, model_type):\n",
    "    \"\"\"\n",
    "    Compute the output of a neural network model.\n",
    "\n",
    "    Args:\n",
    "        x: Input vector (1, x) including bias.\n",
    "        Theta: Tuple of network parameters (W1, w2).\n",
    "        model_type: Type of model (1, 2, or 3).\n",
    "\n",
    "    Returns:\n",
    "        y: Network output.\n",
    "        h: Hidden layer output, or None.\n",
    "    \"\"\"\n",
    "\n",
    "    W1, w2 = Theta # w2 is None if model_type is 1 or 2\n",
    "\n",
    "    if model_type == 1:\n",
    "        # One-layer network (Linear Model)\n",
    "        y = numpy.dot(x, W1)\n",
    "        return y, None # To make this consistent when model_type is 3\n",
    "\n",
    "    elif model_type == 2:\n",
    "        # One-layer network with tanh activation\n",
    "        y = numpy.tanh(numpy.dot(x, W1))\n",
    "        y = numpy.tanh(numpy.dot(x, W1))\n",
    "        return y, None\n",
    "\n",
    "    elif model_type == 3:\n",
    "        # Two-layer network with tanh activation\n",
    "        a_ = numpy.dot(W1, x)  # W1 shape: (K, D+1); x shape: (D+1,)\n",
    "        h_ = numpy.tanh(a_)\n",
    "        h = numpy.concatenate((numpy.array([1.]), h_))  # prepend bias term; h shape becomes (K+1,)\n",
    "        y = numpy.dot(h, w2)  # w2 shape should be (K+1,)\n",
    "        return y, h\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "g5XL_ohAE-Zh"
   },
   "source": [
    "#### Test 1: Sanity Check\n",
    "\n",
    "We select a specific number of hidden neurons and create the weights accordingly, using all zeros in the first layer and all ones in the second. The test case below ensures that the function from Task 1 actually returns $11$ for those weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ku3Sy5fzj8YH",
    "outputId": "45b07ac7-686d-432e-8d09-1342af087687"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N1 test passed.\n",
      "N2 test passed.\n",
      "N3 test passed.\n"
     ]
    }
   ],
   "source": [
    "# Define test parameters\n",
    "K = 20\n",
    "D = 1\n",
    "Theta_one_layer = [numpy.ones(D+1),None]\n",
    "Theta_two_layer = [numpy.zeros((K, D+1)), numpy.ones(K+1)]\n",
    "x = numpy.random.rand(D+1)\n",
    "\n",
    "# Sanity check for N1\n",
    "y1, _ = network(x, Theta_one_layer, 1)\n",
    "assert abs(numpy.sum(x) - y1) < 1e-6\n",
    "print(\"N1 test passed.\")\n",
    "\n",
    "# Sanity check for N2\n",
    "y2, _ = network(x, Theta_one_layer, 2)\n",
    "assert abs(numpy.tanh(numpy.sum(x)) - y2) < 1e-6\n",
    "print(\"N2 test passed.\")\n",
    "\n",
    "# Sanity check for N3\n",
    "y3, _ = network(x, Theta_two_layer, 3)\n",
    "assert abs(1.0 - y3) < 1e-6\n",
    "print(\"N3 test passed.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "-fW7_KzcE-Zi"
   },
   "source": [
    "### Gradient Implementation\n",
    "#### Task 2.2: Gradient Computation\n",
    "\n",
    "\n",
    "Implementation of a function that returns the gradient as defined for a given dataset $X=\\{(\\vec x^{[n]}, t^{[n]})\\}$, given weight(s) $\\Theta$, model_type ($N_1$, $N_2$, or $N_3$), and $\\lambda$ parameter for weight decay.\n",
    "\n",
    "---\n",
    "Note:\n",
    "\n",
    "We should make sure that both parts of the gradient are computed for $N_3$ (since $\\Theta=(\\mathbf W^{(1)},\\vec w^{(2)})$ here).\n",
    "\n",
    "This is a very slow implementation. We will see how to speed this up in the next lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3Vl_dYxcW-VD"
   },
   "outputs": [],
   "source": [
    "def compute_gradient(X, Theta, model_type, lambda_=1.):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the loss function w.r.t. the weights for each model type.\n",
    "    \"\"\"\n",
    "    # Unpack parameters\n",
    "    W1, w2 = Theta  # w2 is None if model_type is 1 or 2\n",
    "    \n",
    "    # Initialize gradients with same shape as parameters\n",
    "    dW1 = numpy.zeros_like(W1, dtype=float)\n",
    "    if model_type in [1, 2]:\n",
    "        dw2 = None\n",
    "    else:\n",
    "        dw2 = numpy.zeros_like(w2, dtype=float)\n",
    "    \n",
    "    # Iterate over dataset\n",
    "    for x, t in X:\n",
    "        y, h = network(x, Theta, model_type)\n",
    "        if model_type == 1:\n",
    "            # Update via element-wise multiplication\n",
    "            dW1 += (y - t) * x\n",
    "        elif model_type == 2:\n",
    "            dW1 += (1 - y**2) * (y - t) * x\n",
    "        else:\n",
    "            # For model N3: update W1 using only the neurons (excluding the bias in h)\n",
    "            dW1 += numpy.outer(h[1:], (y - t))\n",
    "            # Update second layer; h has full bias (shape K+1)\n",
    "            dw2 += (y - t) * h\n",
    "\n",
    "    # Add weight decay term\n",
    "    dW1 += lambda_ * W1\n",
    "    if model_type == 3:\n",
    "        dw2 += lambda_ * w2\n",
    "    \n",
    "    return dW1, dw2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "FqAMeoy5E-aG"
   },
   "source": [
    "#### Task 2.3: Gradient Descent\n",
    "\n",
    "The procedure of gradient descent is the repeated application of two steps.\n",
    "\n",
    "1. The gradient of loss $\\nabla_{\\Theta}\\mathcal J^{L_2}$ is computed based on the current value of the parameters $\\Theta$.\n",
    "2. The weights are updated by moving a small step in the direction of the negative gradient:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\Theta = \\Theta - \\eta \\nabla_{\\Theta}\\mathcal J\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "As a stopping criterion, we select the number of training epochs to be 10000.\n",
    "\n",
    "Implementation of a function that performs gradient descent for a given dataset $X$, given initial parameters $\\Theta$, a given learning rate $\\eta$, model_type ($N_1$, $N_2$, or $N_3$), and $\\lambda$ parameter for weight decay, and returns the optimized parameters $\\Theta^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Hx6Jjs2e2CFX"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X, Theta, eta, model_type, lambda_=1.):\n",
    "    epochs = 10000\n",
    "    # Convert Theta to a list for mutable updates.\n",
    "    params = list(Theta)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        # Pass Theta as a tuple to compute_gradient if needed.\n",
    "        dW1, dw2 = compute_gradient(X, tuple(params), model_type, lambda_)\n",
    "        if model_type in [1, 2]:\n",
    "            params[0] = params[0] - eta * dW1\n",
    "        else:\n",
    "            params[0] = params[0] - eta * dW1\n",
    "            params[1] = params[1] - eta * dw2\n",
    "\n",
    "    # Return as a tuple or list, depending on your usage.\n",
    "    return tuple(params)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5gEeh7N8E-aH"
   },
   "source": [
    "### Datasets\n",
    "\n",
    "#### Task 2.4: Data Samples\n",
    "\n",
    "In total, we will test our gradient descent function with three different datasets. Particularly, we approximate\n",
    "\n",
    "1. $X_1: t = \\cos(3x)$ for $x\\in[-2,2]$\n",
    "2. $X_2: t = e^{-x^2}$ for $x\\in[-1,1]$\n",
    "3. $X_3: t = x^5 + 3x^4 - 6x^3 -12x^2 + 5x + 129$ for $x\\in[-4,2.5]$\n",
    "\n",
    "Generate dataset $X_1$,  for $N=60$ samples randomly drawn from range $x\\in[-2,2]$. Generate data $X_2$ for $N=50$ samples randomly drawn from range $x\\in[-1,1]$. Generate dataset $X_3$ for $N=200$ samples randomly drawn from range $x\\in[-4,2.5]$. Implement all three datasets as lists of tuples: $\\{(\\vec x^{[n]}, t^{[n]})\\mid 1\\leq n\\leq N\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1DdPBymdcNXx"
   },
   "outputs": [],
   "source": [
    "# Dataset X1: t = cos(3x) for x in [-2, 2] with N=60 samples\n",
    "N1 = 60\n",
    "x_vals_X1 = numpy.random.uniform(-2, 2, N1)\n",
    "X1 = [(numpy.array([1, x]), numpy.cos(3 * x)) for x in x_vals_X1]\n",
    "\n",
    "# Dataset X2: t = exp(-x^2) for x in [-1, 1] with N=50 samples\n",
    "N2 = 50\n",
    "x_vals_X2 = numpy.random.uniform(-1, 1, N2)\n",
    "X2 = [(numpy.array([1, x]), numpy.exp(-(x**2))) for x in x_vals_X2]\n",
    "\n",
    "# Dataset X3: t = x^5 + 3x^4 - 6x^3 - 12x^2 + 5x + 129 for x in [-4, 2.5] with N=200 samples\n",
    "N3 = 200\n",
    "x_vals_X3 = numpy.random.uniform(-4, 2.5, N3)\n",
    "X3 = [\n",
    "    (numpy.array([1, x]), x**5 + 3 * x**4 - 6 * x**3 - 12 * x**2 + 5 * x + 129)\n",
    "    for x in x_vals_X3\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "6tGZqaiUE-aH"
   },
   "source": [
    "#### Test 2: Sanity Check\n",
    "\n",
    "The test case below ensures that the elements of each generated dataset are tuples with two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WrneyBJLE-aI",
    "outputId": "96be9dcb-0709-45d3-b209-5a226b429045"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "assert all(\n",
    "    isinstance(x, (tuple,list)) and\n",
    "    len(x) == 2 and\n",
    "    isinstance(x[0], (tuple,list,numpy.ndarray)) and\n",
    "    len(x[0] == 2) and\n",
    "    isinstance(x[1], float)\n",
    "    for X in (X1, X2, X3)\n",
    "    for x in X\n",
    ")\n",
    "\n",
    "print('Test passed!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "v0p9mC4YE-aI"
   },
   "source": [
    "### Function Approximation\n",
    "Finally, we want to make use of our gradient descent implementation to approximate our functions. In order to see our success, we want to plot the functions together with the data.\n",
    "\n",
    "#### Task 2.5: Define hidden Neurons\n",
    "How many hidden neurons will we need for $N_3$? Use the answers from Task 1.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OLichgq7cNXy"
   },
   "outputs": [],
   "source": [
    "# Define the number of neurons for each target function based on your discussion\n",
    "K1 = 5\n",
    "K2 = 10\n",
    "K3 = 15"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "1fvx31eyE-aI"
   },
   "source": [
    "#### Task 2.6: Random Parameters\n",
    "\n",
    "For each of the networks, randomly initialize the parameters $\\Theta_1,\\Theta_2,\\Theta_3\\in[-1,1]$ for each of the datasets.\n",
    "\n",
    "For $N_3$, use the number of hidden neurons estimated in Task 1.2 and implemented in Task 2.5.\n",
    "\n",
    "---\n",
    "Note:\n",
    "\n",
    "  1. You can use `numpy.random.uniform` to initialize the weights.\n",
    "  2. Make sure that the weight matrices are instantiated in the correct dimensions.\n",
    "  3. Theta should always have two elements. The second element can be `None` for one-layer networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Aq768_chcNXy"
   },
   "outputs": [],
   "source": [
    "Theta_N1 = {\n",
    "    \"X1\": (numpy.random.uniform(-1, 1, 2), None),\n",
    "    \"X2\": (numpy.random.uniform(-1, 1, 2), None),\n",
    "    \"X3\": (numpy.random.uniform(-1, 1, 2), None),\n",
    "}\n",
    "\n",
    "Theta_N2 = {\n",
    "    \"X1\": (numpy.random.uniform(-1, 1, 2), None),\n",
    "    \"X2\": (numpy.random.uniform(-1, 1, 2), None),\n",
    "    \"X3\": (numpy.random.uniform(-1, 1, 2), None),\n",
    "}\n",
    "\n",
    "Theta_N3 = {\n",
    "    \"X1\": (numpy.random.uniform(-1, 1, (K1, 2)), numpy.random.uniform(-1, 1, (K1 + 1,))),\n",
    "    \"X2\": (numpy.random.uniform(-1, 1, (K2, 2)), numpy.random.uniform(-1, 1, (K2 + 1,))),\n",
    "    \"X3\": (numpy.random.uniform(-1, 1, (K3, 2)), numpy.random.uniform(-1, 1, (K3 + 1,))),\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "1jCM2nyRE-aI"
   },
   "source": [
    "#### Task 2.7: Run Gradient Descent\n",
    "\n",
    "For each network, call gradient descent function from Task 2.3 using the datasets $X_1, X_2, X_3$, the according created parameters $\\Theta_1,\\Theta_2,\\Theta_3$. Store the resulting optimized weights $\\Theta_1^*, \\Theta_2^*, \\Theta_3^*$.\n",
    "\n",
    "Based on your chosen learning rates $\\eta$ and weight decay parameter $\\lambda$, you may need to optimize them for these functions. Do you see any differences? What are the best learning rates that you can find?\n",
    "\n",
    "---\n",
    "<span style=\"color:red\">WARNING: Depending on the implementation, this might run for several minutes!</span>\n",
    "\n",
    "---\n",
    "Note:\n",
    "\n",
    "1. Start with $\\eta=0.1$ and play around with the learning rate improve adaptation.\n",
    "2. $\\eta=0.1$ is too large for $X_3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "I4sjPUH_cNXz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\boris\\AppData\\Local\\Temp\\ipykernel_35340\\2867201336.py:20: RuntimeWarning: overflow encountered in add\n",
      "  dW1 += (y - t) * x\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([nan, nan]), None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N1\n",
    "# Call gradient descent function using the datasets and initial weights that you created above\n",
    "# Choose appropriate learning rates for each function\n",
    "gradient_descent(X1, Theta_N1[\"X1\"], 0.01, 1)\n",
    "gradient_descent(X2, Theta_N1[\"X2\"], 0.01, 1)\n",
    "gradient_descent(X3, Theta_N1[\"X3\"], 0.01, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Uio-gjhi3GBx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 4.90128899, -0.15034129]), None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N2\n",
    "# Call gradient descent function using the datasets and initial weights that you created above\n",
    "# Choose appropriate learning rates for each function\n",
    "gradient_descent(X1, Theta_N2[\"X1\"], 0.01, 2)\n",
    "gradient_descent(X2, Theta_N2[\"X2\"], 0.01, 2)\n",
    "gradient_descent(X3, Theta_N2[\"X3\"], 0.01, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dCPF-Q3C3GJk"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\boris\\AppData\\Local\\Temp\\ipykernel_35340\\2867201336.py:25: RuntimeWarning: overflow encountered in add\n",
      "  dW1 += numpy.outer(h[1:], (y - t))\n",
      "C:\\Users\\boris\\AppData\\Local\\Temp\\ipykernel_35340\\2867201336.py:27: RuntimeWarning: overflow encountered in add\n",
      "  dw2 += (y - t) * h\n"
     ]
    }
   ],
   "source": [
    "# N3\n",
    "# Call gradient descent function using the datasets and initial weights that you created above\n",
    "# Choose appropriate learning rates for each function\n",
    "gradient_descent(X1, Theta_N3[\"X1\"], 0.001, 3)\n",
    "gradient_descent(X2, Theta_N3[\"X2\"], 0.001, 3)\n",
    "gradient_descent(X3, Theta_N3[\"X3\"], 0.001, 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8_C3TseE-aJ"
   },
   "source": [
    "### Data and Function Plotting\n",
    "\n",
    "### Task 2.8: Plotting Function\n",
    "\n",
    "Implement a plotting function that takes a given dataset $X$, given parameters $\\Theta$, model_type, and a defined range $R=[\\min,\\max]$. Each data sample $(x^{[n]},t^{[n]})$ of the dataset is plotted as an $''x''$. In order to plot the function that is approximated by the network, generate sufficient equally-spaced input values $x\\in R$, compute the network output $y$ for these inputs, and plot them with a line.\n",
    "\n",
    "---\n",
    "Note:\n",
    "\n",
    "  1. The dataset $X$ is defined as above, a list of tuples $(\\vec x, t)$.\n",
    "  2. Each input in the dataset is defined as $\\vec x = (1,x)^T$.\n",
    "  3. Equidistant points can be obtained via `numpy.arange`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T17:56:52.482871Z",
     "start_time": "2025-03-10T17:56:52.230790Z"
    },
    "id": "6sII26VJcNXz"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "def plot(X, Theta, model_type, R):\n",
    "  # first, plot data samples\n",
    "  x = [x_[1] for x_, _ in X]\n",
    "  t = [t_ for _, t_ in X]\n",
    "  pyplot.plot(x, t, \"bo\", label=\"data\")\n",
    "  pyplot.xlabel(\"x\")\n",
    "  pyplot.ylabel(\"t\")\n",
    "\n",
    "\n",
    "  # define equidistant points from min (R[0]) to max (R[1]) to evaluate the network\n",
    "  x = numpy.arange(R[0], R[1], 0.01)\n",
    "  x = numpy.array([[1, x_] for x_ in x])\n",
    "\n",
    "  # compute the network outputs for these values\n",
    "  y = [network(x_, Theta, model_type)[0] for x_ in x]\n",
    "  # plot network approximation\n",
    "  pyplot.plot(x[:, 1],y,\"k-\", label=\"network\", color=\"red\")\n",
    "  pyplot.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "YXcO5e-sE-aJ"
   },
   "source": [
    "#### Task 2.9: Plot Three Functions\n",
    "\n",
    "For each of the datasets and their optimized parameters, call the plotting function from Task 2.8. Use range $R=[-3,3]$ for dataset $X_1$, range $R=[-2,2]$ for $X_2$, and range $R=[-5,4]$ for dataset $X_3$.\n",
    "\n",
    "Note that the first element of range $R$ should be the lowest $x$-location, and the second element of $R$ is the highest value for $x$.\n",
    "\n",
    "Repeat for three networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CY3YGkXgcNXz"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example for Network N1 (one-layer linear network)\n",
    "fig = plt.figure(figsize=(10, 3))\n",
    "# Plot for dataset X1 with range R=[-3, 3]\n",
    "plt.subplot(131)\n",
    "plot(X1, Theta=Theta_N1[\"X1\"], model_type=1, R=[-3, 3])\n",
    "plt.title(\"X1 - N1\")\n",
    "\n",
    "# Plot for dataset X2 with range R=[-2, 2]\n",
    "plt.subplot(132)\n",
    "plot(X2, Theta=Theta_N1[\"X2\"], model_type=1, R=[-2, 2])\n",
    "plt.title(\"X2 - N1\")\n",
    "\n",
    "# Plot for dataset X3 with range R=[-5, 4]\n",
    "plt.subplot(133)\n",
    "plot(X3, Theta=Theta_N1[\"X3\"], model_type=1, R=[-5, 4])\n",
    "plt.title(\"X3 - N1\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Example for Network N2 (one-layer nonlinear network)\n",
    "fig = plt.figure(figsize=(10, 3))\n",
    "plt.subplot(131)\n",
    "plot(X1, Theta=Theta_N2[\"X1\"], model_type=2, R=[-3, 3])\n",
    "plt.title(\"X1 - N2\")\n",
    "plt.subplot(132)\n",
    "plot(X2, Theta=Theta_N2[\"X2\"], model_type=2, R=[-2, 2])\n",
    "plt.title(\"X2 - N2\")\n",
    "plt.subplot(133)\n",
    "plot(X3, Theta=Theta_N2[\"X3\"], model_type=2, R=[-5, 4])\n",
    "plt.title(\"X3 - N2\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Example for Network N3 (two-layer network)\n",
    "fig = plt.figure(figsize=(10, 3))\n",
    "plt.subplot(131)\n",
    "plot(X1, Theta=Theta_N3[\"X1\"], model_type=3, R=[-3, 3])\n",
    "plt.title(\"X1 - N3\")\n",
    "plt.subplot(132)\n",
    "plot(X2, Theta=Theta_N3[\"X2\"], model_type=3, R=[-2, 2])\n",
    "plt.title(\"X2 - N3\")\n",
    "plt.subplot(133)\n",
    "plot(X3, Theta=Theta_N3[\"X3\"], model_type=3, R=[-5, 4])\n",
    "plt.title(\"X3 - N3\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
